{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cee0297f",
   "metadata": {},
   "source": [
    "# Sentiment-Analysis-using-Python\n",
    "One of the applications of text mining is sentiment analysis. Most of the data is getting generated in textual format and in the past few years. Improvement is a continuous process and many product based companies leverage these text mining techniques to examine the sentiments of the customers to find about what they can improve in the product. This information also helps them to understand the trend and demand of the end user which results in Customer satisfaction.\n",
    "\n",
    "As text mining is a vast concept, the article is divided into two subchapters. The main focus of this article will be calculating two scores: sentiment polarity and subjectivity using python. The range of polarity is from -1 to 1(negative to positive) and will tell us if the text contains positive or negative feedback. Most companies prefer to stop their analysis here but in our second article, we will try to extend our analysis by creating some labels out of these scores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d168d99",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22c84545",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import numpy as np \n",
    "import random\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from wordcloud import WordCloud\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8590c551",
   "metadata": {},
   "source": [
    "## Import the Data and convert the sample data to a csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb249b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data\n",
    "df=pd.read_json(r'C:\\Users\\alexa\\nit\\Sentiment-Analysis-using-Python\\Sample Data.txt', lines=True) \n",
    "\n",
    "#convert the sample data to a csv file\n",
    "df.to_csv(r'C:\\Users\\alexa\\nit\\Sentiment-Analysis-using-Python\\Sample Data.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fe9b0b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_type</th>\n",
       "      <th>url</th>\n",
       "      <th>date</th>\n",
       "      <th>content</th>\n",
       "      <th>renderedContent</th>\n",
       "      <th>id</th>\n",
       "      <th>user</th>\n",
       "      <th>replyCount</th>\n",
       "      <th>retweetCount</th>\n",
       "      <th>likeCount</th>\n",
       "      <th>...</th>\n",
       "      <th>media</th>\n",
       "      <th>retweetedTweet</th>\n",
       "      <th>quotedTweet</th>\n",
       "      <th>inReplyToTweetId</th>\n",
       "      <th>inReplyToUser</th>\n",
       "      <th>mentionedUsers</th>\n",
       "      <th>coordinates</th>\n",
       "      <th>place</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>cashtags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>snscrape.modules.twitter.Tweet</td>\n",
       "      <td>https://twitter.com/theworkingboat/status/1460...</td>\n",
       "      <td>2021-11-16 09:54:20+00:00</td>\n",
       "      <td>⭐ THANK YOU ⭐\\n\\nOur popular #Sunday night pub...</td>\n",
       "      <td>⭐ THANK YOU ⭐\\n\\nOur popular #Sunday night pub...</td>\n",
       "      <td>1460546769096425476</td>\n",
       "      <td>{'_type': 'snscrape.modules.twitter.User', 'us...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>[{'_type': 'snscrape.modules.twitter.Photo', '...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'_type': 'snscrape.modules.twitter.User', 'u...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[Sunday, TheWorkingBoat, Falmouth]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>snscrape.modules.twitter.Tweet</td>\n",
       "      <td>https://twitter.com/WeHoLove/status/1460523193...</td>\n",
       "      <td>2021-11-16 08:20:39+00:00</td>\n",
       "      <td>Ahhh big stars in #gayweho #redressparty #mick...</td>\n",
       "      <td>Ahhh big stars in #gayweho #redressparty #mick...</td>\n",
       "      <td>1460523193395933188</td>\n",
       "      <td>{'_type': 'snscrape.modules.twitter.User', 'us...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>[{'_type': 'snscrape.modules.twitter.Photo', '...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[gayweho, redressparty, mickysweho, Sunday, th...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>snscrape.modules.twitter.Tweet</td>\n",
       "      <td>https://twitter.com/bdsrated/status/1460518587...</td>\n",
       "      <td>2021-11-16 08:02:21+00:00</td>\n",
       "      <td>Without God\\nour week would be\\nSINDAY\\nMOURND...</td>\n",
       "      <td>Without God\\nour week would be\\nSINDAY\\nMOURND...</td>\n",
       "      <td>1460518587639025673</td>\n",
       "      <td>{'_type': 'snscrape.modules.twitter.User', 'us...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>[{'_type': 'snscrape.modules.twitter.Photo', '...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'_type': 'snscrape.modules.twitter.User', 'u...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[CHURCH, SUNDAY, BDMS, SOMEWHEREINCAVITE, BILL]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>snscrape.modules.twitter.Tweet</td>\n",
       "      <td>https://twitter.com/maggietranquila/status/146...</td>\n",
       "      <td>2021-11-16 07:34:44+00:00</td>\n",
       "      <td>#awesome #Attitude #Motivation #commitment #Su...</td>\n",
       "      <td>#awesome #Attitude #Motivation #commitment #Su...</td>\n",
       "      <td>1460511638856114185</td>\n",
       "      <td>{'_type': 'snscrape.modules.twitter.User', 'us...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[awesome, Attitude, Motivation, commitment, Su...</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>snscrape.modules.twitter.Tweet</td>\n",
       "      <td>https://twitter.com/BadalonaCC/status/14605071...</td>\n",
       "      <td>2021-11-16 07:16:46+00:00</td>\n",
       "      <td>@BadalonaCC #Sunday training with @omaree02\\n\\...</td>\n",
       "      <td>@BadalonaCC #Sunday training with @omaree02\\n\\...</td>\n",
       "      <td>1460507115982925826</td>\n",
       "      <td>{'_type': 'snscrape.modules.twitter.User', 'us...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>[{'_type': 'snscrape.modules.twitter.Video', '...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'_type': 'snscrape.modules.twitter.User', 'u...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>[Sunday]</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            _type  \\\n",
       "0  snscrape.modules.twitter.Tweet   \n",
       "1  snscrape.modules.twitter.Tweet   \n",
       "2  snscrape.modules.twitter.Tweet   \n",
       "3  snscrape.modules.twitter.Tweet   \n",
       "4  snscrape.modules.twitter.Tweet   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://twitter.com/theworkingboat/status/1460...   \n",
       "1  https://twitter.com/WeHoLove/status/1460523193...   \n",
       "2  https://twitter.com/bdsrated/status/1460518587...   \n",
       "3  https://twitter.com/maggietranquila/status/146...   \n",
       "4  https://twitter.com/BadalonaCC/status/14605071...   \n",
       "\n",
       "                       date  \\\n",
       "0 2021-11-16 09:54:20+00:00   \n",
       "1 2021-11-16 08:20:39+00:00   \n",
       "2 2021-11-16 08:02:21+00:00   \n",
       "3 2021-11-16 07:34:44+00:00   \n",
       "4 2021-11-16 07:16:46+00:00   \n",
       "\n",
       "                                             content  \\\n",
       "0  ⭐ THANK YOU ⭐\\n\\nOur popular #Sunday night pub...   \n",
       "1  Ahhh big stars in #gayweho #redressparty #mick...   \n",
       "2  Without God\\nour week would be\\nSINDAY\\nMOURND...   \n",
       "3  #awesome #Attitude #Motivation #commitment #Su...   \n",
       "4  @BadalonaCC #Sunday training with @omaree02\\n\\...   \n",
       "\n",
       "                                     renderedContent                   id  \\\n",
       "0  ⭐ THANK YOU ⭐\\n\\nOur popular #Sunday night pub...  1460546769096425476   \n",
       "1  Ahhh big stars in #gayweho #redressparty #mick...  1460523193395933188   \n",
       "2  Without God\\nour week would be\\nSINDAY\\nMOURND...  1460518587639025673   \n",
       "3  #awesome #Attitude #Motivation #commitment #Su...  1460511638856114185   \n",
       "4  @BadalonaCC #Sunday training with @omaree02\\n\\...  1460507115982925826   \n",
       "\n",
       "                                                user  replyCount  \\\n",
       "0  {'_type': 'snscrape.modules.twitter.User', 'us...           0   \n",
       "1  {'_type': 'snscrape.modules.twitter.User', 'us...           0   \n",
       "2  {'_type': 'snscrape.modules.twitter.User', 'us...           0   \n",
       "3  {'_type': 'snscrape.modules.twitter.User', 'us...           0   \n",
       "4  {'_type': 'snscrape.modules.twitter.User', 'us...           0   \n",
       "\n",
       "   retweetCount  likeCount  ...  \\\n",
       "0             0          0  ...   \n",
       "1             0          3  ...   \n",
       "2             0          0  ...   \n",
       "3             0          0  ...   \n",
       "4             2          4  ...   \n",
       "\n",
       "                                               media  retweetedTweet  \\\n",
       "0  [{'_type': 'snscrape.modules.twitter.Photo', '...             NaN   \n",
       "1  [{'_type': 'snscrape.modules.twitter.Photo', '...             NaN   \n",
       "2  [{'_type': 'snscrape.modules.twitter.Photo', '...             NaN   \n",
       "3                                               None             NaN   \n",
       "4  [{'_type': 'snscrape.modules.twitter.Video', '...             NaN   \n",
       "\n",
       "  quotedTweet inReplyToTweetId inReplyToUser  \\\n",
       "0        None              NaN          None   \n",
       "1        None              NaN          None   \n",
       "2        None              NaN          None   \n",
       "3        None              NaN          None   \n",
       "4        None              NaN          None   \n",
       "\n",
       "                                      mentionedUsers coordinates place  \\\n",
       "0  [{'_type': 'snscrape.modules.twitter.User', 'u...        None  None   \n",
       "1                                               None        None  None   \n",
       "2  [{'_type': 'snscrape.modules.twitter.User', 'u...        None  None   \n",
       "3                                               None        None  None   \n",
       "4  [{'_type': 'snscrape.modules.twitter.User', 'u...        None  None   \n",
       "\n",
       "                                            hashtags  cashtags  \n",
       "0                 [Sunday, TheWorkingBoat, Falmouth]      None  \n",
       "1  [gayweho, redressparty, mickysweho, Sunday, th...      None  \n",
       "2    [CHURCH, SUNDAY, BDMS, SOMEWHEREINCAVITE, BILL]      None  \n",
       "3  [awesome, Attitude, Motivation, commitment, Su...      None  \n",
       "4                                           [Sunday]      None  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20362506",
   "metadata": {},
   "source": [
    "# Data Preprocessing \n",
    "In the above-given problem statement we have performed various pre-processing steps on the dataset that mainly dealt with removing stopwords, removing emojis. The text document is then converted into the lowercase for better generalization.\n",
    "\n",
    "Subsequently, the punctuations were cleaned and removed thereby reducing the unnecessary noise from the dataset. After that, we have also removed the repeating characters from the words along with removing the URLs as they do not have any significant importance.\n",
    "\n",
    "At last, we then performed Stemming(reducing the words to their derived stems) and Lemmatization(reducing the derived words to their root form known as lemma) for better results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096c8b88",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "568fb132",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e91ba5a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5642, 28)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd5f623b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\alexa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemma = WordNetLemmatizer()\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9537c998",
   "metadata": {},
   "source": [
    "#### Making statement text in lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "debc0124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    ⭐ thank you ⭐\\n\\nour popular #sunday night pub...\n",
       "1    ahhh big stars in #gayweho #redressparty #mick...\n",
       "2    without god\\nour week would be\\nsinday\\nmournd...\n",
       "3    #awesome #attitude #motivation #commitment #su...\n",
       "4    @badalonacc #sunday training with @omaree02\\n\\...\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['content']=df['content'].str.lower()\n",
    "df['content'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d010da",
   "metadata": {},
   "source": [
    "#### Cleaning and removing the above stop words list from the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e1eef56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    ⭐ thank ⭐ popular #sunday night pub quizzes hu...\n",
       "1    ahhh big stars #gayweho #redressparty #mickysw...\n",
       "2    without god week would sinday mournday tearsda...\n",
       "3    #awesome #attitude #motivation #commitment #su...\n",
       "4    @badalonacc #sunday training @omaree02 @cricke...\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "STOPWORDS = set(stopwords.words('english'))\n",
    "def cleaning_stopwords(text):\n",
    "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
    "df['content'] = df['content'].apply(lambda text: cleaning_stopwords(text))\n",
    "df['content'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d35773",
   "metadata": {},
   "source": [
    "### Removing punctuation, numbers and special characters\n",
    "#### This will replace everything except characters and hashtags with spaces. \"[^a-zA-Z#]\" this regular expression means everything except alphabets and hashtags."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96aef39c",
   "metadata": {},
   "source": [
    "#### Cleaning and removing punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "886dba5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    ⭐ thank ⭐ popular sunday night pub quizzes hug...\n",
       "1    ahhh big stars gayweho redressparty mickysweho...\n",
       "2    without god week would sinday mournday tearsda...\n",
       "3    awesome attitude motivation commitment sunday ...\n",
       "4    badalonacc sunday training omaree02 cricketpun...\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "english_punctuations = string.punctuation\n",
    "punctuations_list = english_punctuations\n",
    "def cleaning_punctuations(text):\n",
    "    translator = str.maketrans('', '', punctuations_list)\n",
    "    return text.translate(translator)\n",
    "df['content']= df['content'].apply(lambda x: cleaning_punctuations(x))\n",
    "df['content'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a219fcf",
   "metadata": {},
   "source": [
    "#### Cleaning and removing repeating characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "068cd29a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    ⭐ thank ⭐ popular sunday night pub quizzes hug...\n",
       "1    ahhh big stars gayweho redressparty mickysweho...\n",
       "2    without god week would sinday mournday tearsda...\n",
       "3    awesome attitude motivation commitment sunday ...\n",
       "4    badalonacc sunday training omaree02 cricketpun...\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cleaning_repeating_char(text):\n",
    "    return re.sub(r'(.)1+', r'1', text)\n",
    "df['content'] = df['content'].apply(lambda x: cleaning_repeating_char(x))\n",
    "df['content'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23aa6535",
   "metadata": {},
   "source": [
    "#### Cleaning and removing URL’s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d76bee36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    ⭐ thank ⭐ popular sunday night pub quizzes hug...\n",
       "1    ahhh big stars gayweho redressparty mickysweho...\n",
       "2    without god week would sinday mournday tearsda...\n",
       "3    awesome attitude motivation commitment sunday ...\n",
       "4    badalonacc sunday training omaree02 cricketpun...\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cleaning_URLs(data):\n",
    "    return re.sub('((www.[^s]+)|(https?://[^s]+))',' ',data)\n",
    "df['content'] = df['content'].apply(lambda x: cleaning_URLs(x))\n",
    "df['content'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6b77e0",
   "metadata": {},
   "source": [
    "#### Cleaning and removing Numeric numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6a98f42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    ⭐ thank ⭐ popular sunday night pub quizzes hug...\n",
       "1    ahhh big stars gayweho redressparty mickysweho...\n",
       "2    without god week would sinday mournday tearsda...\n",
       "3    awesome attitude motivation commitment sunday ...\n",
       "4    badalonacc sunday training omaree  cricketpunt...\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cleaning_numbers(data):\n",
    "    return re.sub('[0-9]+', ' ', data)\n",
    "df['content'] = df['content'].apply(lambda x: cleaning_numbers(x))\n",
    "df['content'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54062db4",
   "metadata": {},
   "source": [
    "### Remove short words\n",
    "#### We remove those words which are of little or no use. So, we will select the length of words which we want to remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31ac502d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    thank popular sunday night pub quizzes huge su...\n",
       "1    ahhh big stars gayweho redressparty mickysweho...\n",
       "2    without god week would sinday mournday tearsda...\n",
       "3    awesome attitude motivation commitment sunday ...\n",
       "4    badalonacc sunday training omaree cricketpuntc...\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def transform_text(text):\n",
    "    return ' '.join([word for word in text.split() if len(word) > 2])\n",
    "df['content'] = df['content'].apply(lambda x: transform_text(x))\n",
    "df['content'].head() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8735a77",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "Tokenization is a way to split the strings into a list of words. In this example you’ll use the Natural Language Toolkit which has built-in functions for tokenization.\n",
    "we can also use regex to tokenize it but it is a bit difficult. Though it gives you more control over our text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8178737",
   "metadata": {},
   "source": [
    "#### Getting tokenization of tweet text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f8efc6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [thank, popular, sunday, night, pub, quizzes, ...\n",
       "1    [ahhh, big, stars, gayweho, redressparty, mick...\n",
       "2    [without, god, week, would, sinday, mournday, ...\n",
       "3    [awesome, attitude, motivation, commitment, su...\n",
       "4    [badalonacc, sunday, training, omaree, cricket...\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function which directly tokenize the tweet data\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tt = TweetTokenizer()\n",
    "df['content']=df['content'].apply(tt.tokenize)\n",
    "df['content'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe05a17d",
   "metadata": {},
   "source": [
    "#### Applying Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc631259",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [thank, popular, sunday, night, pub, quizzes, ...\n",
       "1    [ahhh, big, stars, gayweho, redressparty, mick...\n",
       "2    [without, god, week, would, sinday, mournday, ...\n",
       "3    [awesome, attitude, motivation, commitment, su...\n",
       "4    [badalonacc, sunday, training, omaree, cricket...\n",
       "Name: content, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "st = nltk.PorterStemmer()\n",
    "def stemming_on_text(data):\n",
    "    text = [st.stem(word) for word in data]\n",
    "    return data\n",
    "df['content']= df['content'].apply(lambda x: stemming_on_text(x))\n",
    "df['content'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090acdfd",
   "metadata": {},
   "source": [
    "#### Applying Lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "474632b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\alexa\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "947b1976",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93momw-1.4\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('omw-1.4')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/omw-1.4\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\alexa/nltk_data'\n    - 'C:\\\\Users\\\\alexa\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\nltk_data'\n    - 'C:\\\\Users\\\\alexa\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\alexa\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\alexa\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\corpus\\util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 84\u001b[0m     root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mzip_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93momw-1.4\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('omw-1.4')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/omw-1.4.zip/omw-1.4/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\alexa/nltk_data'\n    - 'C:\\\\Users\\\\alexa\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\nltk_data'\n    - 'C:\\\\Users\\\\alexa\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\alexa\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\alexa\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m     text \u001b[38;5;241m=\u001b[39m [lm\u001b[38;5;241m.\u001b[39mlemmatize(word) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n\u001b[1;32m----> 5\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlemmatizer_on_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\series.py:4433\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4323\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4324\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4325\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4328\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4329\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4330\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4331\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4332\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4431\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4432\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4433\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\apply.py:1088\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1084\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m   1085\u001b[0m     \u001b[38;5;66;03m# if we are a string, try to dispatch\u001b[39;00m\n\u001b[0;32m   1086\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[1;32m-> 1088\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\apply.py:1143\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1137\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[0;32m   1138\u001b[0m         \u001b[38;5;66;03m# error: Argument 2 to \"map_infer\" has incompatible type\u001b[39;00m\n\u001b[0;32m   1139\u001b[0m         \u001b[38;5;66;03m# \"Union[Callable[..., Any], str, List[Union[Callable[..., Any], str]],\u001b[39;00m\n\u001b[0;32m   1140\u001b[0m         \u001b[38;5;66;03m# Dict[Hashable, Union[Union[Callable[..., Any], str],\u001b[39;00m\n\u001b[0;32m   1141\u001b[0m         \u001b[38;5;66;03m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[39;00m\n\u001b[0;32m   1142\u001b[0m         \u001b[38;5;66;03m# \"Callable[[Any], Any]\"\u001b[39;00m\n\u001b[1;32m-> 1143\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1144\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1145\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   1146\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1147\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1150\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1151\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\_libs\\lib.pyx:2870\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Input \u001b[1;32mIn [17]\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      3\u001b[0m     text \u001b[38;5;241m=\u001b[39m [lm\u001b[38;5;241m.\u001b[39mlemmatize(word) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n\u001b[1;32m----> 5\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mlemmatizer_on_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      6\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mhead()\n",
      "Input \u001b[1;32mIn [17]\u001b[0m, in \u001b[0;36mlemmatizer_on_text\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlemmatizer_on_text\u001b[39m(data):\n\u001b[1;32m----> 3\u001b[0m     text \u001b[38;5;241m=\u001b[39m [lm\u001b[38;5;241m.\u001b[39mlemmatize(word) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "Input \u001b[1;32mIn [17]\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlemmatizer_on_text\u001b[39m(data):\n\u001b[1;32m----> 3\u001b[0m     text \u001b[38;5;241m=\u001b[39m [\u001b[43mlm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlemmatize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\stem\\wordnet.py:45\u001b[0m, in \u001b[0;36mWordNetLemmatizer.lemmatize\u001b[1;34m(self, word, pos)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlemmatize\u001b[39m(\u001b[38;5;28mself\u001b[39m, word: \u001b[38;5;28mstr\u001b[39m, pos: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;124;03m\"\"\"Lemmatize `word` using WordNet's built-in morphy function.\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;124;03m    Returns the input word unchanged if it cannot be found in WordNet.\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;124;03m    :return: The lemma of `word`, for the given `pos`.\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m     lemmas \u001b[38;5;241m=\u001b[39m \u001b[43mwn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_morphy\u001b[49m(word, pos)\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(lemmas, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m lemmas \u001b[38;5;28;01melse\u001b[39;00m word\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\corpus\\util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\corpus\\util.py:89\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[1;32m---> 89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m# This is where the magic happens!  Transform ourselves into\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;66;03m# the corpus by modifying our own __dict__ and __class__ to\u001b[39;00m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m# match that of the corpus.\u001b[39;00m\n\u001b[0;32m     95\u001b[0m args, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py:1176\u001b[0m, in \u001b[0;36mWordNetCorpusReader.__init__\u001b[1;34m(self, root, omw_reader)\u001b[0m\n\u001b[0;32m   1172\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1173\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe multilingual functions are not available with this Wordnet version\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1174\u001b[0m     )\n\u001b[0;32m   1175\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1176\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprovenances \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43momw_prov\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1178\u001b[0m \u001b[38;5;66;03m# A cache to store the wordnet data of multiple languages\u001b[39;00m\n\u001b[0;32m   1179\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lang_data \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28mlist\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py:1285\u001b[0m, in \u001b[0;36mWordNetCorpusReader.omw_prov\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1283\u001b[0m provdict \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m   1284\u001b[0m provdict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meng\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1285\u001b[0m fileids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_omw_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfileids\u001b[49m()\n\u001b[0;32m   1286\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fileid \u001b[38;5;129;01min\u001b[39;00m fileids:\n\u001b[0;32m   1287\u001b[0m     prov, langfile \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplit(fileid)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\corpus\\util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\corpus\\util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m             root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[1;32m---> 86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[0;32m     89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\corpus\\util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 81\u001b[0m         root \u001b[38;5;241m=\u001b[39m \u001b[43mnltk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubdir\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     83\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93momw-1.4\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('omw-1.4')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/omw-1.4\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\alexa/nltk_data'\n    - 'C:\\\\Users\\\\alexa\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\nltk_data'\n    - 'C:\\\\Users\\\\alexa\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\alexa\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\alexa\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "lm = nltk.WordNetLemmatizer()\n",
    "def lemmatizer_on_text(data):\n",
    "    text = [lm.lemmatize(word) for word in data]\n",
    "    return data\n",
    "df['content'] = df['content'].apply(lambda x: lemmatizer_on_text(x))\n",
    "df['content'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292682c2",
   "metadata": {},
   "source": [
    "## Subjectivity and polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a03355",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a function to get the subjectivity\n",
    "def getSubjectivity(text):\n",
    "    return TextBlob(text).sentiment.subjectivity\n",
    "\n",
    "#create a function to get the polarity\n",
    "def getpolarity(text):\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "\n",
    "#create two new columns\n",
    "df['subjectivity']=df['content'].apply(getSubjectivity)\n",
    "df['polarity']=df['content'].apply(getpolarity)\n",
    "\n",
    "#show the new datafraem with the new columns\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d061823",
   "metadata": {},
   "source": [
    "## Compute the negative, neutral and positive analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4b8c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a function to compute the negative, neutral and positive analysis\n",
    "def getAnalysis(score):\n",
    "    if score<0:\n",
    "        return 'negative'\n",
    "    elif score==0:\n",
    "        return 'neutral'\n",
    "    else:\n",
    "        return 'positive'\n",
    "    \n",
    "df['analysis']=df['polarity'].apply(getAnalysis)\n",
    "\n",
    "#show dataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987880e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create two new dataframe all of the positive text\n",
    "df_positive = df[df['analysis'] == 'positive']\n",
    "\n",
    "\n",
    "# create two new dataframe all of the negative text\n",
    "df_negative = df[df['analysis'] == 'negative']\n",
    "\n",
    "\n",
    "# create two new dataframe all of the neutral text\n",
    "df_neutral=df[df['analysis'] == 'neutral']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d256016",
   "metadata": {},
   "source": [
    "#### Count the number of positive, negative, neutral reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f741654a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_counts = df.analysis.value_counts()\n",
    "tb_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa352a94",
   "metadata": {},
   "source": [
    "# Data Exploration\n",
    "## Let's form a WordCloud\n",
    "### A wordcloud is a visualization wherein the most frequent words appear in large size and the less frequent words appear in smaller sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960ff369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing all tweets\n",
    "\n",
    "all_words = \" \".join(sent for sent in df['content'])\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "wordcloud = WordCloud(width=800, height=500, random_state=42, max_font_size=100).generate(all_words)\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f289e323",
   "metadata": {},
   "source": [
    "## Positive tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a3de00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualizing all positive tweets\n",
    "\n",
    "all_pos_words = \" \".join(sent for sent in df_positive['content'])\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "wordcloud = WordCloud(width=800, height=500, random_state=42, max_font_size=100).generate(all_pos_words)\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d24b1e8",
   "metadata": {},
   "source": [
    "## Negative tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e74f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing all negative tweets\n",
    "\n",
    "all_neg_words = \" \".join(sent for sent in df_negative['content'])\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "wordcloud = WordCloud(width=800, height=500, random_state=42, max_font_size=100).generate(all_neg_words)\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6233bf",
   "metadata": {},
   "source": [
    "## Neutral tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c3f71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing all neutral tweets\n",
    "\n",
    "all_neu_words = \" \".join(sent for sent in df_neutral['content'])\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "wordcloud = WordCloud(width=800, height=500, random_state=42, max_font_size=100).generate(all_neu_words)\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e98fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the polarity and subjectivity\n",
    "plt.figure(figsize=(8,6))\n",
    "for i in range(0,df.shape[0]):\n",
    "    plt.scatter(df['polarity'][i],df['subjectivity'][i],color='Blue')\n",
    "\n",
    "plt.title('Sentiment Analysis')\n",
    "plt.xlabel('Polarity')\n",
    "plt.ylabel('Subjectivity')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae839d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the percentage of positive tweets\n",
    "print(\"Positive tweets\",round((df_positive.shape[0]/df.shape[0])*100,1),\"%\")\n",
    "# Get the percentage of negative tweets\n",
    "print(\"Negative tweets\",round((df_negative.shape[0]/df.shape[0])*100,1),\"%\")\n",
    "# Get the percentage of neutral tweets\n",
    "print(\"Neutral tweets\",round((df_neutral.shape[0]/df.shape[0])*100,1),\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fd1104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the value counts\n",
    "\n",
    "df['analysis'].value_counts()\n",
    "\n",
    "#plot and visualize the counts\n",
    "plt.title('Sentiment Analysis')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Count')\n",
    "df['analysis'].value_counts().plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339a5e3d",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "we can see that maximum percentage of neutral tweets 48.4% , minimum percentage of negative tweets 7.5% and Avg percentage of positive tweets 44.1%."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "bbc3c3d932324566a9bf4b4a52ddf64063695fc3adbf25b3fda92572428493bb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
